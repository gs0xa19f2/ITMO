# WebCrawler

## Описание

`WebCrawler` — это многопоточный веб-краулер, который обходит ссылки на веб-страницах до заданной глубины. Его цель — демонстрация работы с многопоточностью и сетевыми запросами.

---

## Основные особенности

- Поддержка многопоточной загрузки веб-страниц.
- Ограничение количества подключений на один хост.
- Поддержка глубины обхода ссылок.
- Управление задачами с помощью `ExecutorService`.

---

## API

### Основные методы
- **`download(String url, int depth)`**: Загружает страницу и ссылки на ней до указанной глубины.
- **`download(String url, int depth, Set<String> excludes)`**: То же, что и `download`, но с исключением определённых ссылок.
- **`close()`**: Завершает работу краулера и освобождает ресурсы.
